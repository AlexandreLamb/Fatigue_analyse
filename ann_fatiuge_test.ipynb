{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorboard.plugins.hparams import api as hp\n"
   ]
  },
  {
   "source": [
    "## Load Data and Create Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t_1       float64\nt_2       float64\nt_3       float64\nt_4       float64\nt_5       float64\nt_6       float64\nt_7       float64\nt_8       float64\nt_9       float64\nt_10      float64\ntarget      int64\ndtype: object\n                t_1           t_2           t_3           t_4           t_5  \\\ncount  12223.000000  12223.000000  12223.000000  12223.000000  12223.000000   \nmean       0.316937      0.316948      0.316949      0.316958      0.316948   \nstd        0.024264      0.024234      0.024221      0.024199      0.024177   \nmin        0.180592      0.180592      0.180592      0.180592      0.180592   \n25%        0.304438      0.304438      0.304414      0.304438      0.304418   \n50%        0.319253      0.319253      0.319280      0.319319      0.319298   \n75%        0.333979      0.333977      0.333979      0.333972      0.333962   \nmax        0.401511      0.400485      0.400485      0.400249      0.400249   \n\n                t_6           t_7           t_8           t_9          t_10  \\\ncount  12223.000000  12223.000000  12223.000000  12223.000000  12223.000000   \nmean       0.316932      0.316911      0.316883      0.316863      0.316847   \nstd        0.024178      0.024173      0.024160      0.024164      0.024165   \nmin        0.180592      0.180592      0.180592      0.180592      0.212515   \n25%        0.304414      0.304397      0.304414      0.304382      0.304397   \n50%        0.319253      0.319253      0.319253      0.319253      0.319253   \n75%        0.333953      0.333953      0.333922      0.333866      0.333885   \nmax        0.400249      0.400249      0.400249      0.400249      0.400249   \n\n             target  \ncount  12223.000000  \nmean       0.372904  \nstd        0.483597  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        1.000000  \nmax        1.000000  \n        t_1       t_2       t_3       t_4       t_5       t_6       t_7  \\\n0  0.401511  0.394261  0.400485  0.393686  0.377596  0.387566  0.387599   \n1  0.394261  0.400485  0.393686  0.377596  0.387566  0.387599  0.368915   \n2  0.400485  0.393686  0.377596  0.387566  0.387599  0.368915  0.392312   \n3  0.393686  0.377596  0.387566  0.387599  0.368915  0.392312  0.400249   \n4  0.377596  0.387566  0.387599  0.368915  0.392312  0.400249  0.333498   \n\n        t_8       t_9      t_10  target  \n0  0.368915  0.392312  0.400249       0  \n1  0.392312  0.400249  0.333498       0  \n2  0.400249  0.333498  0.282115       0  \n3  0.333498  0.282115  0.263787       0  \n4  0.282115  0.263787  0.246496       0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/stage_data_out/dataset/dataset_ann/ear_test.csv\", index_col=0)\n",
    "print(df.dtypes)\n",
    "print(df.describe())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop('target')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(df), target.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every feature: ['t_1', 't_2', 't_3', 't_4', 't_5', 't_6', 't_7', 't_8', 't_9', 't_10']\nA batch of t_1: tf.Tensor(0.4015111547859027, shape=(), dtype=float64)\nA batch of targets: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in dataset.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of t_1:', feature_batch['t_1'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "source": [
    "## Splitting, Shuffing, Batching  data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Splitting and Shuffling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full dataset size: 12223\nTrain dataset size: 8556\nVal dataset size: 1833\nTest dataset size: 1833\n"
     ]
    }
   ],
   "source": [
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "dataset = dataset.shuffle(buffer_size = dataset_size)\n",
    "\n",
    "train_size = int(0.7*dataset_size)\n",
    "val_size = int(0.15*dataset_size)\n",
    "test_size = int(0.15*dataset_size)\n",
    "\n",
    "train = dataset.take(train_size)\n",
    "val = dataset.skip(train_size)\n",
    "val = dataset.take(val_size)\n",
    "test = dataset.skip(train_size + val_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"Full dataset size:\", dataset_size)\n",
    "print(\"Train dataset size:\", train_size)\n",
    "print(\"Val dataset size:\", val_size)\n",
    "print(\"Test dataset size:\", test_size)"
   ]
  },
  {
   "source": [
    "### Shuffling, Batching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(buffer_size = train_size)\n",
    "train = train.batch(BATCH_SIZE)\n",
    "\n",
    "val = val.shuffle(buffer_size = val_size)\n",
    "val = val.batch(BATCH_SIZE)\n",
    "\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "source": [
    "## Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# POnly if we have features with different scale\n",
    "def normalize_numerical_features(df, features):\n",
    "  def get_mean_std(x):\n",
    "    return df[x].mean(), df[x].std()\n",
    "  for column in features: \n",
    "    mean, std = get_mean_std(column)\n",
    "    def z_score(col):\n",
    "      return (col - mean)/std    \n",
    "    def _numeric_column_normalized(column_name, normalizer_fn):\n",
    "      return tf.feature_column.numeric_column(column_name, normalizer_fn=normalizer_fn)\n",
    "    return _numeric_column_normalized(column,z_score)\n",
    "  \n",
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "\n",
    "def make_numerical_feature_col(numerical_column, normalize = False):\n",
    "    for column_name in numerical_column:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=column_name)\n",
    "        if normalize : \n",
    "            normalization_layer = get_normalization_layer(column_name, train)\n",
    "            encoded_numeric_col = normalization_layer(numeric_col) \n",
    "        else : \n",
    "            encoded_numeric_col = feature_column.numeric_column(column_name)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numerical_features = [\"t_1\",\"t_2\",\"t_3\",\"t_4\",\"t_5\",\"t_6\",\"t_7\",\"t_8\",\"t_9\",\"t_10\"]\n",
    "all_inputs, encoded_features = make_numerical_feature_col(numerical_features, normalize = True)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hyper Parameter tuning (Hparams & tensor board)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Define log dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"tensorboard/logs/fit/tunning/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\""
   ]
  },
  {
   "source": [
    "### Define model parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([256,512]))\n",
    "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([256,512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','elu']))\n",
    "HP_ACTIVATION_OUTPUT = hp.HParam('activation_output', hp.Discrete(['sigmoid']))\n",
    "\n",
    "METRIC_CATEGORICAL_ACCURACY = \"categorical_accuracy\"\n",
    "METRIC_BINARY_ACCURACY = \"binary_accuracy\"\n",
    "METRIC_CATEGORICAL_CROSSENTROPY = \"categorical_crossentropy\"\n",
    "METRIC_BINARY_CROSSENTROPY = \"binary_crossentropy\"\n",
    "METRIC_MSE = \"mean_squared_error\"\n",
    "\n",
    "NUMBER_OF_TARGET = 2\n",
    "metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"]\n"
   ]
  },
  {
   "source": [
    "### Initialize hyper parameter for the log"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS_1, HP_NUM_UNITS_2, HP_DROPOUT, HP_ACTIVATION, HP_ACTIVATION_OUTPUT, HP_OPTIMIZER],\n",
    "    metrics=[ hp.Metric(METRIC_CATEGORICAL_ACCURACY, display_name='Categorical Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_ACCURACY, display_name='Binary Accuracy'),\n",
    "              hp.Metric(METRIC_CATEGORICAL_CROSSENTROPY, display_name='Categorical Cross Entropy Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_CROSSENTROPY, display_name='Binary Cross Entropy'),\n",
    "              hp.Metric(METRIC_MSE, display_name='MSE'),\n",
    "    ],\n",
    "  )"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Define the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(all_features, hparams):\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(all_features)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_1],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(NUMBER_OF_TARGET, activation=hparams[HP_ACTIVATION_OUTPUT])(x)\n",
    "    model = tf.keras.Model(all_inputs,output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(all_features, hparams):\n",
    "    model(all_features, hparams)\n",
    "  \n",
    "    model.compile(\n",
    "        optimizer = hparams[HP_OPTIMIZER],\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics = [\"categorical_accuracy\",\"binary_accuracy\",\"categorical_crossentropy\",\"binary_crossentropy\",\"mean_squared_error\"],\n",
    "    )\n",
    "    model.fit(\n",
    "        train, \n",
    "        validation_data= val,\n",
    "        epochs=10,\n",
    "        shuffle=True,\n",
    "        verbose =1,\n",
    "        callbacks=[ \n",
    "            tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
    "            hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=10),\n",
    "        ]\n",
    "    ) \n",
    "    _, categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = model.evaluate(test)\n",
    "    return categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error"
   ]
  },
  {
   "source": [
    "### Define a method to run the the training and testing model function and logs the paramete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        categorical_accuracy, binary_accuracy, categorical_crossentropy, binary_crossentropy, mean_squared_error = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_CATEGORICAL_ACCURACY, categorical_accuracy, step=1)\n",
    "        tf.summary.scalar(METRIC_BINARY_ACCURACY, binary_accuracy, step=1)\n",
    "        tf.summary.scalar(METRIC_CATEGORICAL_CROSSENTROPY, categorical_crossentropy, step=1)\n",
    "        tf.summary.scalar(METRIC_BINARY_CROSSENTROPY, binary_crossentropy, step=1)\n",
    "        tf.summary.scalar(METRIC_MSE, mean_squared_error, step=1)"
   ]
  },
  {
   "source": [
    "### Tunning the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    " \n",
    "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
    "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "          for activation in HP_ACTIVATION.domain.values:\n",
    "            for activation_output in HP_ACTIVATION_OUTPUT.domain.values:\n",
    "              hparams = {\n",
    "                HP_NUM_UNITS_1: num_units_1,\n",
    "                HP_NUM_UNITS_2: num_units_2,\n",
    "                HP_DROPOUT : dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_ACTIVATION_OUTPUT: activation_output\n",
    "              }\n",
    "              run_name = \"run-%d\" % session_num\n",
    "              print('--- Starting trial: %s' % run_name)\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              run(logdir + run_name, hparams)\n",
    "              session_num += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'logs/fit'"
   ]
  }
 ]
}