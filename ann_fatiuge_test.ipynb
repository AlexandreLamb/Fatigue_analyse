{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime"
   ]
  },
  {
   "source": [
    "## Load Data and Create Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ear_l     float64\near_r     float64\near       float64\nTarget      int64\ndtype: object\n               ear_l          ear_r            ear         Target\ncount  116069.000000  116069.000000  116069.000000  116069.000000\nmean        0.305900       0.304856       0.305378       0.732986\nstd         0.033780       0.031586       0.031354       0.442401\nmin         0.112955       0.093567       0.105054       0.000000\n25%         0.286097       0.290172       0.289110       0.000000\n50%         0.311300       0.310224       0.312204       1.000000\n75%         0.329276       0.325553       0.326692       1.000000\nmax         0.440620       0.500000       0.427168       1.000000\n          ear_l     ear_r       ear  Target\nframe                                      \n0.0    0.282051  0.289474  0.285762       0\n1.0    0.294775  0.310811  0.302793       0\n2.0    0.294872  0.281959  0.288415       0\n3.0    0.269142  0.289374  0.279258       0\n4.0    0.277569  0.276817  0.277193       0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/stage_data_out/dataset/Merge_Dataset/Merge_Dataset.csv\", index_col=0)\n",
    "print(df.dtypes)\n",
    "print(df.describe())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TensorSliceDataset shapes: ({ear_l: (), ear_r: (), ear: ()}, ()), types: ({ear_l: tf.float64, ear_r: tf.float64, ear: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "target = df.pop('Target')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(df), target.values))\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every feature: ['ear_l', 'ear_r', 'ear']\nA batch of ear: tf.Tensor(0.2857624831309042, shape=(), dtype=float64)\nA batch of targets: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in dataset.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ear:', feature_batch['ear'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "source": [
    "## Splitting, Shuffing, Batching  data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Splitting and Shuffling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full dataset size: 116069\nTrain dataset size: 81248\nVal dataset size: 17410\nTest dataset size: 17410\n"
     ]
    }
   ],
   "source": [
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "dataset = dataset.shuffle(buffer_size = dataset_size)\n",
    "\n",
    "train_size = int(0.7*dataset_size)\n",
    "val_size = int(0.15*dataset_size)\n",
    "test_size = int(0.15*dataset_size)\n",
    "\n",
    "train = dataset.take(train_size)\n",
    "val = dataset.skip(train_size)\n",
    "val = dataset.take(val_size)\n",
    "test = dataset.skip(train_size + val_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"Full dataset size:\", dataset_size)\n",
    "print(\"Train dataset size:\", train_size)\n",
    "print(\"Val dataset size:\", val_size)\n",
    "print(\"Test dataset size:\", test_size)"
   ]
  },
  {
   "source": [
    "### Shuffling, Batching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(buffer_size = train_size)\n",
    "train = train.batch(BATCH_SIZE)\n",
    "\n",
    "val = val.shuffle(buffer_size = val_size)\n",
    "val = val.batch(BATCH_SIZE)\n",
    "\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "source": [
    "## Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# POnly if we have features with different scale\n",
    "def normalize_numerical_features(df, features):\n",
    "  def get_mean_std(x):\n",
    "    return df[x].mean(), df[x].std()\n",
    "  for column in features: \n",
    "    mean, std = get_mean_std(column)\n",
    "    def z_score(col):\n",
    "      return (col - mean)/std    \n",
    "    def _numeric_column_normalized(column_name, normalizer_fn):\n",
    "      return tf.feature_column.numeric_column(column_name, normalizer_fn=normalizer_fn)\n",
    "    return _numeric_column_normalized(column,z_score)\n",
    "  \n",
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "\n",
    "def make_numerical_feature_col(numerical_column, normalize = False):\n",
    "    for column_name in numerical_column:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=column_name)\n",
    "        if normalize : \n",
    "            normalization_layer = get_normalization_layer(column_name, train)\n",
    "            encoded_numeric_col = normalization_layer(numeric_col) \n",
    "        else : \n",
    "            encoded_numeric_col = feature_column.numeric_column(column_name)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numerical_features = [\"ear\",\"ear_l\",\"ear_r\"]\n",
    "all_inputs, encoded_features = make_numerical_feature_col(numerical_features, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hyper Parameter tuning (Hparams & tensor board)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Define log dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"tensorboard/logs/fit/tunning/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\""
   ]
  },
  {
   "source": [
    "### Define model parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([32,64,128,256,512]))\n",
    "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([32,64,128,256,512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam's,'adadelta','sgd']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','elu']))\n",
    "HP_ACTIVATION_OUTPUT = hp.HParam('activation_output', hp.Discrete(['sigmoid']))\n",
    "\n",
    "\n",
    "METRIC_BINARY_ACCURACY = \"binary_accuracy\"\n",
    "METRIC_BINARY_CROSSENTROPY = \"binary_crossentropy\"\n",
    "METRIC_MSE = \"mean_squared_error\"\n",
    "\n",
    "NUMBER_OF_TARGET = 1\n",
    "metrics = [\"binary_accuracy\",\"binary_crossentropy\",\"mean_squared_error\"]\n"
   ]
  },
  {
   "source": [
    "### Initialize hyper parameter for the log"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS_1, HP_NUM_UNITS_2, HP_DROPOUT, HP_ACTIVATION, HP_ACTIVATION_OUTPUT, HP_OPTIMIZER],\n",
    "    metrics=[ hp.Metric(METRIC_BINARY_ACCURACY, display_name='Binary Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_CROSSENTROPY, display_name='Binary Cross Entropy'),\n",
    "              hp.Metric(METRIC_MSE, display_name='MSE'),\n",
    "    ],\n",
    "  )"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "source": [
    "### Define the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(hparams):\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(all_features)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_1],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(NUMBER_OF_TARGET, activation=hparams[HP_ACTIVATION_OUTPUT])(x)\n",
    "    model = tf.keras.Model(all_inputs,output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = modeling(hparams)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer = hparams[HP_OPTIMIZER],\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics = [\"binary_accuracy\",\"binary_crossentropy\",\"mean_squared_error\"],\n",
    "    )\n",
    "    model.fit(\n",
    "        train, \n",
    "        validation_data= val,\n",
    "        epochs=10,\n",
    "        shuffle=True,\n",
    "        verbose =1,\n",
    "        callbacks=[ \n",
    "            tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
    "            hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=10),\n",
    "        ]\n",
    "    ) \n",
    "    _, binary_accuracy, binary_crossentropy, mean_squared_error = model.evaluate(test)\n",
    "    return binary_accuracy, binary_crossentropy, mean_squared_error"
   ]
  },
  {
   "source": [
    "### Define a method to run the the training and testing model function and logs the paramete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        binary_accuracy, binary_crossentropy, mean_squared_error = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_BINARY_ACCURACY, binary_accuracy, step=1)\n",
    "        tf.summary.scalar(METRIC_BINARY_CROSSENTROPY, binary_crossentropy, step=1)\n",
    "        tf.summary.scalar(METRIC_MSE, mean_squared_error, step=1)"
   ]
  },
  {
   "source": [
    "### Tunning the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units_1': 32, 'num_units_2': 32, 'dropout': 0.2, 'optimizer': 'adadelta', 'activation': 'elu', 'activation_output': 'sigmoid'}\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ear (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ear_l (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ear_r (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 1)            3           ear[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 1)            3           ear_l[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           ear_r[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3)            0           normalization[0][0]              \n",
      "                                                                 normalization_1[0][0]            \n",
      "                                                                 normalization_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 3)            12          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           128         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32)           128         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           1056        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32)           128         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            33          batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,494\n",
      "Trainable params: 1,351\n",
      "Non-trainable params: 143\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "2539/2539 [==============================] - 6s 2ms/step - loss: 0.8349 - binary_accuracy: 0.4839 - binary_crossentropy: 0.8349 - mean_squared_error: 0.3007 - val_loss: 0.7104 - val_binary_accuracy: 0.4492 - val_binary_crossentropy: 0.7104 - val_mean_squared_error: 0.2584\n",
      "Epoch 2/10\n",
      " 573/2539 [=====>........................] - ETA: 2s - loss: 0.8016 - binary_accuracy: 0.5003 - binary_crossentropy: 0.8016 - mean_squared_error: 0.2897"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cd1d41ecf2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m               \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a18dec11ac44>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# record the values used in this trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_BINARY_ACCURACY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_BINARY_CROSSENTROPY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-70b1801e2268>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    " \n",
    "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
    "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "          for activation in HP_ACTIVATION.domain.values:\n",
    "            for activation_output in HP_ACTIVATION_OUTPUT.domain.values:\n",
    "              hparams = {\n",
    "                HP_NUM_UNITS_1: num_units_1,\n",
    "                HP_NUM_UNITS_2: num_units_2,\n",
    "                HP_DROPOUT : dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_ACTIVATION_OUTPUT: activation_output\n",
    "              }\n",
    "              run_name = \"run-%d\" % session_num\n",
    "              print('--- Starting trial: %s' % run_name)\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              run(logdir + run_name, hparams)\n",
    "              session_num += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 69428), started 0:02:10 ago. (Use '!kill 69428' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    " %reload_ext tensorboard\n",
    " %tensorboard --logdir 'tensorboard/logs/fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[NbConvertApp] Converting notebook ann_fatiuge_test.ipynb to script\n",
      "[NbConvertApp] Writing 8502 bytes to ann_fatiuge_test.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ann_fatiuge_test.ipynb"
   ]
  }
 ]
}