{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import io\n",
    "import itertools\n",
    "import numpy as np \n",
    "import json\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import datetime\n"
   ]
  },
  {
   "source": [
    "## Load Data and Create Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ear_l     float64\near_r     float64\near       float64\nTarget      int64\ndtype: object\n               ear_l          ear_r            ear         Target\ncount  116069.000000  116069.000000  116069.000000  116069.000000\nmean        0.305900       0.304856       0.305378       0.732986\nstd         0.033780       0.031586       0.031354       0.442401\nmin         0.112955       0.093567       0.105054       0.000000\n25%         0.286097       0.290172       0.289110       0.000000\n50%         0.311300       0.310224       0.312204       1.000000\n75%         0.329276       0.325553       0.326692       1.000000\nmax         0.440620       0.500000       0.427168       1.000000\n          ear_l     ear_r       ear  Target\nframe                                      \n0.0    0.282051  0.289474  0.285762       0\n1.0    0.294775  0.310811  0.302793       0\n2.0    0.294872  0.281959  0.288415       0\n3.0    0.269142  0.289374  0.279258       0\n4.0    0.277569  0.276817  0.277193       0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ear_l     float64\near_r     float64\near       float64\nTarget      int64\ndtype: object\n               ear_l          ear_r            ear         Target\ncount  116069.000000  116069.000000  116069.000000  116069.000000\nmean        0.305900       0.304856       0.305378       0.732986\nstd         0.033780       0.031586       0.031354       0.442401\nmin         0.112955       0.093567       0.105054       0.000000\n25%         0.286097       0.290172       0.289110       0.000000\n50%         0.311300       0.310224       0.312204       1.000000\n75%         0.329276       0.325553       0.326692       1.000000\nmax         0.440620       0.500000       0.427168       1.000000\n          ear_l     ear_r       ear  Target\nframe                                      \n0.0    0.282051  0.289474  0.285762       0\n1.0    0.294775  0.310811  0.302793       0\n2.0    0.294872  0.281959  0.288415       0\n3.0    0.269142  0.289374  0.279258       0\n4.0    0.277569  0.276817  0.277193       0\n{'Fatigue': frame\n0.0        0\n1.0        0\n2.0        0\n3.0        0\n4.0        0\n          ..\n30461.0    1\n30462.0    1\n30463.0    1\n30464.0    1\n30465.0    1\nName: Fatigue, Length: 116069, dtype: int64, 'Awake': frame\n0.0        1\n1.0        1\n2.0        1\n3.0        1\n4.0        1\n          ..\n30461.0    0\n30462.0    0\n30463.0    0\n30464.0    0\n30465.0    0\nName: Awake, Length: 116069, dtype: int64}\n<TensorSliceDataset shapes: ({ear_l: (), ear_r: (), ear: ()}, {Fatigue: (), Awake: ()}), types: ({ear_l: tf.float64, ear_r: tf.float64, ear: tf.float64}, {Fatigue: tf.int64, Awake: tf.int64})>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/stage_data_out/dataset/Merge_Dataset/Merge_Dataset.csv\", index_col=0)\n",
    "print(df.dtypes)\n",
    "print(df.describe())\n",
    "print(df.head(5))\n",
    "\n",
    "target = None\n",
    "target = df.pop('Target')\n",
    "target = pd.DataFrame(target)\n",
    "target = target.rename(columns = {\"Target\"  : 'Fatigue'})\n",
    "\n",
    "target['Awake'] = np.where(target['Fatigue'] == 0, 1, 0)\n",
    "print(dict(target))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(df), dict(target)))\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every Feature: ['ear_l', 'ear_r', 'ear']\nEvery Target: ['Fatigue', 'Awake']\nA batch of ear: tf.Tensor(0.2857624831309042, shape=(), dtype=float64)\nA batch of targets: [<tf.Tensor: shape=(), dtype=int64, numpy=0>, <tf.Tensor: shape=(), dtype=int64, numpy=1>]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in dataset.take(1):\n",
    "  print('Every Feature:', list(feature_batch.keys()))\n",
    "  print('Every Target:', list(label_batch.keys()))\n",
    "  print('A batch of ear:', feature_batch['ear'])\n",
    "  print('A batch of targets:', list(label_batch.values()) )"
   ]
  },
  {
   "source": [
    "## Splitting, Shuffing, Batching  data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Splitting and Shuffling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full dataset size: 116069\nTrain dataset size: 81248\nVal dataset size: 17410\nTest dataset size: 17410\n"
     ]
    }
   ],
   "source": [
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "dataset = dataset.shuffle(buffer_size = dataset_size)\n",
    "\n",
    "train_size = int(0.7*dataset_size)\n",
    "val_size = int(0.15*dataset_size)\n",
    "test_size = int(0.15*dataset_size)\n",
    "\n",
    "train = dataset.take(train_size)\n",
    "val = dataset.skip(train_size)\n",
    "val = dataset.take(val_size)\n",
    "test = dataset.skip(train_size + val_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"Full dataset size:\", dataset_size)\n",
    "print(\"Train dataset size:\", train_size)\n",
    "print(\"Val dataset size:\", val_size)\n",
    "print(\"Test dataset size:\", test_size)"
   ]
  },
  {
   "source": [
    "### Shuffling, Batching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(buffer_size = train_size)\n",
    "train = train.batch(BATCH_SIZE)\n",
    "\n",
    "val = val.shuffle(buffer_size = val_size)\n",
    "val = val.batch(BATCH_SIZE)\n",
    "\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "source": [
    "## Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# POnly if we have features with different scale\n",
    "def normalize_numerical_features(df, features):\n",
    "  def get_mean_std(x):\n",
    "    return df[x].mean(), df[x].std()\n",
    "  for column in features: \n",
    "    mean, std = get_mean_std(column)\n",
    "    def z_score(col):\n",
    "      return (col - mean)/std    \n",
    "    def _numeric_column_normalized(column_name, normalizer_fn):\n",
    "      return tf.feature_column.numeric_column(column_name, normalizer_fn=normalizer_fn)\n",
    "    return _numeric_column_normalized(column,z_score)\n",
    "  \n",
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "\n",
    "def make_numerical_feature_col(numerical_column, normalize = False):\n",
    "    for column_name in numerical_column:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=column_name)\n",
    "        if normalize : \n",
    "            normalization_layer = get_normalization_layer(column_name, train)\n",
    "            encoded_numeric_col = normalization_layer(numeric_col) \n",
    "        else : \n",
    "            encoded_numeric_col = feature_column.numeric_column(column_name)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numerical_features = [\"ear\",\"ear_l\",\"ear_r\"]\n",
    "all_inputs, encoded_features = make_numerical_feature_col(numerical_features, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           ear_l     ear_r       ear  Target\n",
      "0       0.282051  0.289474  0.285762       0\n",
      "1       0.294775  0.310811  0.302793       0\n",
      "2       0.294872  0.281959  0.288415       0\n",
      "3       0.269142  0.289374  0.279258       0\n",
      "4       0.277569  0.276817  0.277193       0\n",
      "...          ...       ...       ...     ...\n",
      "116064  0.225338  0.235180  0.230259       1\n",
      "116065  0.258415  0.250970  0.254693       1\n",
      "116066  0.249878  0.270459  0.260168       1\n",
      "116067  0.258046  0.250010  0.254028       1\n",
      "116068  0.279743  0.277760  0.278752       1\n",
      "\n",
      "[116069 rows x 4 columns]\n",
      "30992\n",
      "85077\n",
      "30992\n",
      "30992\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmps4g0kech\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmps4g0kech', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmps4g0kech/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.6931472, step = 1\n",
      "INFO:tensorflow:global_step/sec: 99.4698\n",
      "INFO:tensorflow:loss = 0.69321525, step = 100 (1.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.359\n",
      "INFO:tensorflow:loss = 0.6936373, step = 200 (0.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.703\n",
      "INFO:tensorflow:loss = 0.69426453, step = 300 (0.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.798\n",
      "INFO:tensorflow:loss = 0.69354576, step = 400 (0.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.581\n",
      "INFO:tensorflow:loss = 0.69299823, step = 500 (0.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.72\n",
      "INFO:tensorflow:loss = 0.6938496, step = 600 (0.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.179\n",
      "INFO:tensorflow:loss = 0.69243455, step = 701 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.021\n",
      "INFO:tensorflow:loss = 0.6918207, step = 800 (0.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.082\n",
      "INFO:tensorflow:loss = 0.69271564, step = 900 (0.671 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmps4g0kech/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:Loss for final step: 0.69366634.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearClassifierV2 at 0x7f9a45034cd0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"data/stage_data_out/dataset/Merge_Dataset/Merge_Dataset.csv\")\n",
    "df.pop(\"frame\")\n",
    "print(df)\n",
    "index_list = list(df[df[\"Target\"] == 1].index)\n",
    "index_to_remove = random.sample(index_list, len(index_list) -len(df[df[\"Target\"] == 0]) )\n",
    "print(len(df[df[\"Target\"] == 0]))\n",
    "print(len(df[df[\"Target\"] == 1]))\n",
    "\n",
    "df.drop(index_to_remove, inplace=True)\n",
    "print(len(df[df[\"Target\"] == 0]))\n",
    "print(len(df[df[\"Target\"] == 1]))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.33)\n",
    "\n",
    "FEATURES = [\"ear\", \"ear_l\", \"ear_r\"]\n",
    "LABEL = 'Target'\n",
    "continuous_features = [tf.feature_column.numeric_column(k) for k in FEATURES]\t\t\n",
    "model = tf.estimator.LinearClassifier(continuous_features, n_classes = 2)\n",
    "\n",
    "def get_input_fn(data_set, num_epochs=None, n_batch = 128, shuffle=True):\n",
    "    return tf.compat.v1.estimator.inputs.pandas_input_fn( x=pd.DataFrame({k: data_set[k].values for k in FEATURES}), y = pd.Series(data_set[LABEL].values), batch_size=n_batch, num_epochs=num_epochs, shuffle=shuffle)\n",
    "\n",
    "model.train(input_fn=get_input_fn(df_train, num_epochs=None, n_batch = 128, shuffle=False),steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-04-14T17:23:33Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmps4g0kech/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Inference Time : 4.39245s\n",
      "INFO:tensorflow:Finished evaluation at 2021-04-14-17:23:37\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.4986556, accuracy_baseline = 0.50134444, auc = 0.5212795, auc_precision_recall = 0.5248309, average_loss = 0.69346386, global_step = 1000, label/mean = 0.50134444, loss = 0.6934649, precision = 0.0, prediction/mean = 0.48490652, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmps4g0kech/model.ckpt-1000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.4986556,\n",
       " 'accuracy_baseline': 0.50134444,\n",
       " 'auc': 0.5212795,\n",
       " 'auc_precision_recall': 0.5248309,\n",
       " 'average_loss': 0.69346386,\n",
       " 'label/mean': 0.50134444,\n",
       " 'loss': 0.6934649,\n",
       " 'precision': 0.0,\n",
       " 'prediction/mean': 0.48490652,\n",
       " 'recall': 0.0,\n",
       " 'global_step': 1000}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "model.evaluate(input_fn=get_input_fn(df_test, \n",
    "                                      num_epochs=1,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmps4g0kech/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "20455\n",
      "20455\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[10200,     0],\n",
       "       [10255,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "predicitons = model.predict(input_fn=get_input_fn(df_test, num_epochs=1,n_batch = 128, shuffle=False))\n",
    "predicitons = np.array([np.round(item[\"logistic\"]) for item in predicitons])\n",
    "print(len(predicitons))\n",
    "print(len(list(df_test[\"Target\"])))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(list(df_test[\"Target\"]), predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           ear_l     ear_r       ear  Target\n",
       "frame                                       \n",
       "0.0     0.282051  0.289474  0.285762       0\n",
       "1.0     0.294775  0.310811  0.302793       0\n",
       "2.0     0.294872  0.281959  0.288415       0\n",
       "3.0     0.269142  0.289374  0.279258       0\n",
       "4.0     0.277569  0.276817  0.277193       0\n",
       "...          ...       ...       ...     ...\n",
       "6454.0  0.250850  0.240810  0.245830       0\n",
       "6458.0  0.257576  0.283741  0.270658       0\n",
       "6459.0  0.242424  0.267111  0.254768       0\n",
       "6461.0  0.227273  0.226482  0.226877       0\n",
       "6462.0  0.266468  0.267111  0.266790       0\n",
       "\n",
       "[29627 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ear_l</th>\n      <th>ear_r</th>\n      <th>ear</th>\n      <th>Target</th>\n    </tr>\n    <tr>\n      <th>frame</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>0.282051</td>\n      <td>0.289474</td>\n      <td>0.285762</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>0.294775</td>\n      <td>0.310811</td>\n      <td>0.302793</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>0.294872</td>\n      <td>0.281959</td>\n      <td>0.288415</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>0.269142</td>\n      <td>0.289374</td>\n      <td>0.279258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>0.277569</td>\n      <td>0.276817</td>\n      <td>0.277193</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6454.0</th>\n      <td>0.250850</td>\n      <td>0.240810</td>\n      <td>0.245830</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6458.0</th>\n      <td>0.257576</td>\n      <td>0.283741</td>\n      <td>0.270658</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6459.0</th>\n      <td>0.242424</td>\n      <td>0.267111</td>\n      <td>0.254768</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6461.0</th>\n      <td>0.227273</td>\n      <td>0.226482</td>\n      <td>0.226877</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6462.0</th>\n      <td>0.266468</td>\n      <td>0.267111</td>\n      <td>0.266790</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>29627 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "6123],\n",
    "       [    0, 17091]])"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hyper Parameter tuning (Hparams & tensor board)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Define log dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"tensorboard/logs/fit/tunning/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\""
   ]
  },
  {
   "source": [
    "### Define model parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([32]))\n",
    "HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.5,0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu']))\n",
    "HP_ACTIVATION_OUTPUT = hp.HParam('activation_output', hp.Discrete(['sigmoid']))\n",
    "\n",
    "\n",
    "METRIC_BINARY_ACCURACY = \"binary_accuracy\"\n",
    "METRIC_BINARY_CROSSENTROPY = \"binary_crossentropy\"\n",
    "METRIC_MSE = \"mean_squared_error\"\n",
    "\n",
    "NUMBER_OF_TARGET = 2\n",
    "metrics = [\"binary_accuracy\",\"binary_crossentropy\",\"mean_squared_error\"]\n"
   ]
  },
  {
   "source": [
    "### Initialize hyper parameter for the log"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS_1, HP_NUM_UNITS_2, HP_DROPOUT, HP_ACTIVATION, HP_ACTIVATION_OUTPUT, HP_OPTIMIZER],\n",
    "    metrics=[ hp.Metric(METRIC_BINARY_ACCURACY, display_name='Binary Accuracy'),\n",
    "              hp.Metric(METRIC_BINARY_CROSSENTROPY, display_name='Binary Cross Entropy'),\n",
    "              hp.Metric(METRIC_MSE, display_name='MSE'),\n",
    "    ],\n",
    "  )"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 51,
   "outputs": []
  },
  {
   "source": [
    "### Define the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(hparams):\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(all_features)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_1],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation=hparams[HP_ACTIVATION])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(NUMBER_OF_TARGET, activation=hparams[HP_ACTIVATION_OUTPUT])(x)\n",
    "    model = tf.keras.Model(all_inputs,output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = modeling(hparams)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer = hparams[HP_OPTIMIZER],\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics = [\"binary_accuracy\",\"binary_crossentropy\",\"mean_squared_error\"],\n",
    "    )\n",
    "    model.fit(\n",
    "        train, \n",
    "        validation_data= val,\n",
    "        epochs=30,\n",
    "        shuffle=True,\n",
    "        verbose =1,\n",
    "        callbacks=[ \n",
    "            tf.keras.callbacks.TensorBoard(log_dir = logdir),  # log metrics\n",
    "            hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=10),\n",
    "        ]\n",
    "    ) \n",
    "    model.save(\"tensorboard/\"+str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) + \"/model\")\n",
    "    _, binary_accuracy, binary_crossentropy, mean_squared_error = model.evaluate(test)\n",
    "    return binary_accuracy, binary_crossentropy, mean_squared_error"
   ]
  },
  {
   "source": [
    "### Define a method to run the the training and testing model function and logs the paramete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        binary_accuracy, binary_crossentropy, mean_squared_error = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_BINARY_ACCURACY, binary_accuracy, step=1)\n",
    "        tf.summary.scalar(METRIC_BINARY_CROSSENTROPY, binary_crossentropy, step=1)\n",
    "        tf.summary.scalar(METRIC_MSE, mean_squared_error, step=1)"
   ]
  },
  {
   "source": [
    "### Tunning the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units_1': 32, 'num_units_2': 512, 'dropout': 0.5, 'optimizer': 'adam', 'activation': 'relu', 'activation_output': 'sigmoid'}\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ear (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ear_l (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ear_r (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 1)            3           ear[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 1)            3           ear_l[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           ear_r[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3)            0           normalization[0][0]              \n",
      "                                                                 normalization_1[0][0]            \n",
      "                                                                 normalization_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3)            12          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           128         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32)           128         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          16896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1026        batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 284,951\n",
      "Trainable params: 282,824\n",
      "Non-trainable params: 2,127\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:748 train_step\n        loss = self.compiled_loss(\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:183 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:63 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:586 map_to_output_names\n        raise ValueError('Found unexpected keys that do not correspond '\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['Fatigue', 'Awake']). Expected: ['dense_3']\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cd1d41ecf2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m               \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-a18dec11ac44>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# record the values used in this trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_BINARY_ACCURACY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_BINARY_CROSSENTROPY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-81ff68c29baf>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 696\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    697\u001b[0m             *args, **kwds))\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:748 train_step\n        loss = self.compiled_loss(\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:183 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:63 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    /home/simeon/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:586 map_to_output_names\n        raise ValueError('Found unexpected keys that do not correspond '\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['Fatigue', 'Awake']). Expected: ['dense_3']\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    " target\n",
    "for num_units_1 in HP_NUM_UNITS_1.domain.values:\n",
    "  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "          for activation in HP_ACTIVATION.domain.values:\n",
    "            for activation_output in HP_ACTIVATION_OUTPUT.domain.values:\n",
    "              hparams = {\n",
    "                HP_NUM_UNITS_1: num_units_1,\n",
    "                HP_NUM_UNITS_2: num_units_2,\n",
    "                HP_DROPOUT : dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_ACTIVATION_OUTPUT: activation_output\n",
    "              }\n",
    "              run_name = \"run-%d\" % session_num\n",
    "              print('--- Starting trial: %s' % run_name)\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              run(logdir + run_name, hparams)\n",
    "              session_num += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-2fc7be45cb2a261e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-2fc7be45cb2a261e\");\n          const url = new URL(\"/\", window.location);\n          const port = 8080;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    " %load_ext tensorboard\n",
    " %tensorboard --logdir 'tensorboard/logs/save_log' --port=8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[NbConvertApp] Converting notebook ann_fatiuge_test.ipynb to script\n",
      "[NbConvertApp] Writing 8864 bytes to ann_fatiuge_test.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ann_fatiuge_test.ipynb"
   ]
  }
 ]
}