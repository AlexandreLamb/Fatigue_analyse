{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import io\n",
    "import itertools\n",
    "import numpy as np \n",
    "import json\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ear_l     float64\near_r     float64\near       float64\nTarget      int64\ndtype: object\n               ear_l          ear_r            ear         Target\ncount  116069.000000  116069.000000  116069.000000  116069.000000\nmean        0.305900       0.304856       0.305378       0.732986\nstd         0.033780       0.031586       0.031354       0.442401\nmin         0.112955       0.093567       0.105054       0.000000\n25%         0.286097       0.290172       0.289110       0.000000\n50%         0.311300       0.310224       0.312204       1.000000\n75%         0.329276       0.325553       0.326692       1.000000\nmax         0.440620       0.500000       0.427168       1.000000\n          ear_l     ear_r       ear  Target\nframe                                      \n0.0    0.282051  0.289474  0.285762       0\n1.0    0.294775  0.310811  0.302793       0\n2.0    0.294872  0.281959  0.288415       0\n3.0    0.269142  0.289374  0.279258       0\n4.0    0.277569  0.276817  0.277193       0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/stage_data_out/dataset/Merge_Dataset/Merge_Dataset.csv\", index_col=0)\n",
    "print(df.dtypes)\n",
    "print(df.describe())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TensorSliceDataset shapes: ({ear_l: (), ear_r: (), ear: ()}, ()), types: ({ear_l: tf.float64, ear_r: tf.float64, ear: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "target = df.pop('Target')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(df), target.values))\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every feature: ['ear_l', 'ear_r', 'ear']\nA batch of ear: tf.Tensor(0.2857624831309042, shape=(), dtype=float64)\nA batch of targets: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in dataset.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ear:', feature_batch['ear'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full dataset size: 116069\nTrain dataset size: 81248\nVal dataset size: 17410\nTest dataset size: 17410\n"
     ]
    }
   ],
   "source": [
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "dataset = dataset.shuffle(buffer_size = dataset_size)\n",
    "\n",
    "train_size = int(0.7*dataset_size)\n",
    "val_size = int(0.15*dataset_size)\n",
    "test_size = int(0.15*dataset_size)\n",
    "\n",
    "train = dataset.take(train_size)\n",
    "val = dataset.skip(train_size)\n",
    "val = dataset.take(val_size)\n",
    "test = dataset.skip(train_size + val_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_size = test.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(\"Full dataset size:\", dataset_size)\n",
    "print(\"Train dataset size:\", train_size)\n",
    "print(\"Val dataset size:\", val_size)\n",
    "print(\"Test dataset size:\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(buffer_size = train_size)\n",
    "train = train.batch(BATCH_SIZE)\n",
    "\n",
    "val = val.shuffle(buffer_size = val_size)\n",
    "val = val.batch(BATCH_SIZE)\n",
    "\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# POnly if we have features with different scale\n",
    "def normalize_numerical_features(df, features):\n",
    "  def get_mean_std(x):\n",
    "    return df[x].mean(), df[x].std()\n",
    "  for column in features: \n",
    "    mean, std = get_mean_std(column)\n",
    "    def z_score(col):\n",
    "      return (col - mean)/std    \n",
    "    def _numeric_column_normalized(column_name, normalizer_fn):\n",
    "      return tf.feature_column.numeric_column(column_name, normalizer_fn=normalizer_fn)\n",
    "    return _numeric_column_normalized(column,z_score)\n",
    "  \n",
    "\n",
    "\n",
    "def make_numerical_feature_col(numerical_column, normalize = False):\n",
    "  def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "  return normalizer\n",
    "    for column_name in numerical_column:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=column_name)\n",
    "        if normalize : \n",
    "            normalization_layer = get_normalization_layer(column_name, train)\n",
    "            encoded_numeric_col = normalization_layer(numeric_col) \n",
    "        else : \n",
    "            encoded_numeric_col = feature_column.numeric_column(column_name)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numerical_features = [\"ear\",\"ear_l\",\"ear_r\"]\n",
    "all_inputs, encoded_features = make_numerical_feature_col(numerical_features, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_accuracy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "        train, \n",
    "        validation_data= val,\n",
    "        epochs=30,\n",
    "        shuffle=True,\n",
    "        verbose =1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')]) \n",
    "model.save(\"tensorboard/model/\"+str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) + \"/model_\" + str(session_num))\n"
   ]
  }
 ]
}